{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Installation of library are mentioned here \"\"\"\n",
    "!pip install happytransformer \n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/02/raya1/unix/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.1) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/u/02/raya1/unix/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-05-04 01:29:51.386010: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-04 01:29:51.433790: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-04 01:29:54.355725: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datasets import load_dataset\n",
    "from happytransformer import TTSettings\n",
    "from happytransformer import TTTrainArgs\n",
    "from happytransformer import HappyTextToText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.42k/1.42k [00:00<00:00, 141kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 892M/892M [00:09<00:00, 90.1MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 1.92k/1.92k [00:00<00:00, 2.96MB/s]\n",
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 1.74MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 2.45MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 1.79k/1.79k [00:00<00:00, 1.25MB/s]\n",
      "05/04/2023 01:36:19 - INFO - happytransformer.happy_transformer -   Using model: cuda\n"
     ]
    }
   ],
   "source": [
    "happy_tt = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8297,), (922,), (8297,), (922,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"testing_data_cleaned.csv\")\n",
    "# df\n",
    "# fill in the values of col5 into col6 \n",
    "df['col6'].fillna(df['col5'], inplace=True)\n",
    "X, y = df['col5'].values, df['col6'].values\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_csv(csv_path, X, y):\n",
    "    with open(csv_path, 'w', newline='') as csvfile:\n",
    "        writter = csv.writer(csvfile)\n",
    "        writter.writerow([\"input\", \"target\"])\n",
    "\n",
    "        for i in range(X.shape[0]) :\n",
    "\n",
    "            input_text = \"grammar: \" + X[i]\n",
    "            correction = y[i]\n",
    "\n",
    "            # a few of the cases contain blank strings. \n",
    "            if input_text and correction:\n",
    "                writter.writerow([input_text, correction])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_csv(\"eval.csv\", X_test, y_test)\n",
    "generate_csv(\"train.csv\", X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2023 01:36:30 - INFO - happytransformer.happy_transformer -   Preprocessing evaluating data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /u/02/raya1/unix/.cache/huggingface/datasets/csv/default-b53a2259e1e84363/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 1774.24it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 177.79it/s]\n",
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /u/02/raya1/unix/.cache/huggingface/datasets/csv/default-b53a2259e1e84363/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 151.54it/s]\n",
      "Map:   0%|          | 0/922 [00:00<?, ? examples/s]/u/02/raya1/unix/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "before_result = happy_tt.eval(\"eval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loss: 0.8561544418334961\n"
     ]
    }
   ],
   "source": [
    "print(\"Before loss:\", before_result.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: I and my friend is going to the park.\n",
      "Top k sampling result: I and my friend are going to the park.\n"
     ]
    }
   ],
   "source": [
    "# read in eval.csv\n",
    "text = \"I and my friend is going to the park.\"\n",
    "print(\"Original text:\", text)\n",
    "\n",
    "# top_k_sampling_args = TTSettings(do_sample=True, top_k=120, top_p=0.95, early_stopping=True, min_length=1, max_length=30)\n",
    "input_text = \"grammar: \" + text + \" </s>\"\n",
    "beam_args = TTSettings(num_beams=5, min_length=1, max_length=len(input_text.split()) + 10 , early_stopping=True)\n",
    "result = happy_tt.generate_text(input_text, args=beam_args)\n",
    "print(\"Top k sampling result:\", result.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/02/raya1/unix/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 30\n",
      "i: 60\n",
      "i: 90\n",
      "i: 120\n",
      "i: 150\n",
      "i: 180\n",
      "i: 210\n",
      "i: 240\n",
      "i: 270\n",
      "i: 300\n",
      "i: 330\n",
      "i: 360\n",
      "i: 390\n",
      "i: 420\n",
      "i: 450\n",
      "i: 480\n",
      "i: 510\n",
      "i: 540\n",
      "i: 570\n",
      "i: 600\n",
      "i: 630\n",
      "i: 660\n",
      "i: 690\n",
      "i: 720\n",
      "i: 750\n",
      "i: 780\n",
      "i: 810\n",
      "i: 840\n",
      "i: 870\n",
      "i: 900\n",
      "True Positive 7375\n",
      "True Negative 29\n",
      "False Positive 408\n",
      "False Positive 485\n"
     ]
    }
   ],
   "source": [
    "# iterate through the sentences\n",
    "tp = tn = fp = fn = 0\n",
    "for i, sent in enumerate(X_test) :\n",
    "    if i%30==0:\n",
    "        print('i:',i)\n",
    "    input_text = \"grammar: \" + sent + \" </s>\"\n",
    "    beam_args = TTSettings(num_beams=5, min_length=1, max_length=len(input_text.split()) + 10 , early_stopping=True)\n",
    "    result = happy_tt.generate_text(input_text, args=beam_args)\n",
    "    if result.text == sent:\n",
    "        tn += 1\n",
    "    else:\n",
    "        if sent != y_test[i]:\n",
    "            fn += 1\n",
    "        else:\n",
    "            fp += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive 485\n",
      "True Negative 29\n",
      "False Positive 408\n",
      "False Positive 485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5080662057406243"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tp =len(X_test) - tn  - fn\n",
    "print('True Positive', tp)\n",
    "print('True Negative', tn)\n",
    "print('False Positive', fn)\n",
    "print('False Positive', fp)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "F = (1+0.5**2)*((precision*recall)/((0.5**2 * precision)+recall))\n",
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/02/raya1/unix/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: The cat sat at mat\n",
      "Top k sampling result: The cat sat at matt.\n",
      "Original text: Giant otters is an apex predator\n",
      "Top k sampling result: Giant otters are an apex predator.\n",
      "Original text: I and my friend is going to the park.\n",
      "Top k sampling result: I and my friend are going to the park.\n"
     ]
    }
   ],
   "source": [
    "# The cat sat at mat\n",
    "# Giant otters is an apex predator\n",
    "# I and my friend is going to the park.\n",
    "\n",
    "sentences = [\n",
    "    \"The cat sat at mat\",\n",
    "    \"Giant otters is an apex predator\",\n",
    "    \"I and my friend is going to the park.\"\n",
    "]\n",
    "\n",
    "for sent in sentences :\n",
    "    input_text = \"grammar: \" + sent + \" </s>\"\n",
    "    beam_args = TTSettings(num_beams=5, min_length=1, max_length=len(input_text.split()) + 10 , early_stopping=True)\n",
    "    result = happy_tt.generate_text(input_text, args=beam_args)\n",
    "    print(\"Original text:\", sent)\n",
    "    print(\"Top k sampling result:\", result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
