{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3qIlKkyeTI8",
        "outputId": "970b5aee-746d-441d-b6d9-a8c811229e7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.10/dist-packages (0.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (4.65.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2022.10.31)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2.0.0+cu118)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (1.26.125)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.12.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch-pretrained-bert) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch-pretrained-bert) (3.25.2)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-pretrained-bert) (1.0.1)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.125 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-pretrained-bert) (1.29.125)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-pretrained-bert) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.125->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch-pretrained-bert) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch-pretrained-bert) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.125->boto3->pytorch-pretrained-bert) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-pretrained-bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nSj7X1VWebA8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm , trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fR-wFGhWejkV"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uHKqUW1HfMtG"
      },
      "outputs": [],
      "source": [
        "#Getting CoLA dataset from its official source\n",
        "!wget https://nyu-mll.github.io/CoLA/cola_public_1.1.zip\n",
        "!unzip cola_public_1.1.zip \n",
        "!cp ./cola_public/raw/in_domain_train.tsv .\n",
        "!cp ./cola_public/raw/out_of_domain_dev.tsv .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zCT1cQESfRqU"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('in_domain_train.tsv', delimiter='\\t', header=None, names=['source', 'label', 'label_notes', 'sentence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mORkp0HtfRuY"
      },
      "outputs": [],
      "source": [
        "\n",
        "sentences = df.sentence.values\n",
        "sentences = [\"[CLS]\" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = df.label.values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dKbVYToKfR1D"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
        "tokens = [tokenizer.tokenize(sent) for sent in sentences] #tokenized sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OvZ4UxXJfR3h"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 256\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokens], maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokens]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype =\"long\", truncating=\"post\",padding =\"post\")\n",
        "attention_masks = []\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i > 0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ai03Rkk1fR8G"
      },
      "outputs": [],
      "source": [
        "tr_inputs, val_inputs, tr_labels, val_labels = train_test_split(input_ids, labels, random_state=2023, test_size=0.1)\n",
        "tr_masks, val_masks , _, _ = train_test_split(attention_masks, input_ids, random_state=2023, test_size=0.1)\n",
        "tr_inputs = torch.tensor(tr_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "tr_labels = torch.tensor(tr_labels)\n",
        "val_labels = torch.tensor(val_labels)\n",
        "tr_masks = torch.tensor(tr_masks)\n",
        "val_masks = torch.tensor(val_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qE4vsDMkfR-b"
      },
      "outputs": [],
      "source": [
        "\n",
        "batch_size = 16\n",
        "tr_data = TensorDataset(tr_inputs, tr_masks, tr_labels)\n",
        "tr_sampler = RandomSampler(tr_data)\n",
        "tr_dataloader = DataLoader(tr_data, sampler=tr_sampler, batch_size=batch_size)\n",
        "\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd5dKUnffmZg",
        "outputId": "0939fe7c-c6ee-4b91-a66a-f2064c50e5c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOq1HGW4fmXw",
        "outputId": "0a3ddedb-6809-44c8-a218-29b81cc40b30"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pytorch_pretrained_bert.optimization:t_total value of -1 results in schedule not being applied\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "\n",
        "optimizer_grouped_parameters = [\n",
        "    {   'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "        'weight_decay_rate': 0.01\n",
        "    },\n",
        "    {   'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n",
        "        'weight_decay_rate': 0.00\n",
        "    }]\n",
        "\n",
        "optimizer = BertAdam(optimizer_grouped_parameters, lr=1e-5, warmup=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Z_Lz3J4ffmVa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# calculates the accuracy of our predictions vs labels\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "  pred_flat  = np.argmax(preds , axis=1).flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "  return np.sum(pred_flat == labels_flat)/len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQcp-UCKfmTC",
        "outputId": "e7f1b012-9332-4b90-cb0b-b003bba5a2c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/4 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
            "Epoch:  25%|██▌       | 1/4 [06:39<19:58, 399.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.4855660913577942\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|█████     | 2/4 [13:16<13:16, 398.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.2556514017314896\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch:  75%|███████▌  | 3/4 [19:53<06:37, 397.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.13768620685850447\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 4/4 [26:30<00:00, 397.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.08910816649493922\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8472222222222222\n"
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "train_loss_set = []\n",
        "epochs = 4\n",
        "\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "  model.train()\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "  for step, batch in enumerate(tr_dataloader):\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask , b_labels = batch\n",
        "    optimizer.zero_grad()\n",
        "    loss = model(b_input_ids, token_type_ids=None, \n",
        "                 attention_mask=b_input_mask, labels=b_labels)\n",
        "    \n",
        "    train_loss_set.append(loss.item())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "for batch in val_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  with torch.no_grad():\n",
        "    logits = model(b_input_ids, token_type_ids =None, attention_mask=b_input_mask)\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "  eval_accuracy += tmp_eval_accuracy\n",
        "  nb_eval_steps += 1\n",
        "\n",
        "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LH5s6j7TfmOY"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"out_of_domain_dev.tsv\", delimiter='\\t', header=None,\n",
        "  names=['source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Create sentence) and label lists\n",
        "sentences = df.sentence.values\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = df.label.values\n",
        "\n",
        "tokens = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "MAX_LEN = 256\n",
        "\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokens], maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokens]\n",
        "\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype =\"long\", truncating=\"post\",padding =\"post\")\n",
        "\n",
        "attention_masks = []\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i > 0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "aMPpTew2fSAz"
      },
      "outputs": [],
      "source": [
        "# Prediction on the test set\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "# Predict\n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask , b_labels = batch\n",
        "  # Telling the model not to compute or store gradients,\n",
        "  # saving memory and speeding up prediction\n",
        "  with torch.no_grad():\n",
        "    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to(\"cpu\").numpy()\n",
        "  \n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "BMef6lXoge8a"
      },
      "outputs": [],
      "source": [
        "# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "\n",
        "torch.save(model.state_dict(), 'bert-base-uncased-GED.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zb_rGimfge5L",
        "outputId": "8ec2900a-f6a4-40f3-ef9c-0baecf857b6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp bert-base-uncased-GED.pth './drive/My Drive/Colab Notebooks'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
